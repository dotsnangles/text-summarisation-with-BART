{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot-prep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh7qm7LpX5teeYyR5ievYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/BART-Text-Summarisation/blob/main/chatbot_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXPA7-uDAIHP"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/Poly-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXufVK0iRN3Z",
        "outputId": "cade84d6-c463-4a52-8082-9e0fc8d701b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown -q --folder 1Ipr-aNF5ELMY0HTXAmeV26LlgktKUfmG"
      ],
      "metadata": {
        "id": "LVqpyud7T6Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnEJHV_6YR4k",
        "outputId": "e4b1e9ed-86bb-4fec-a07a-ad5d1367265d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  6 00:10:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ZyBZPWfX5m",
        "outputId": "94804013-05db-4474-a687-cf79bec1285e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 362 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 67.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25h  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets folium==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lrYnocYNH4",
        "outputId": "7aa669ee-0be8-4664-f399-98587d280a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Poly-Encoder'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 129 (delta 41), reused 20 (delta 8), pack-reused 57\u001b[K\n",
            "Receiving objects: 100% (129/129), 40.70 KiB | 1.07 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWnJ129TxQSl",
        "outputId": "508720fc-d438-4be6-e6de-f61cedb5b127"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(32001, 768)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AutoModel\n",
        "from encoder import PolyEncoder\n",
        "from transform import SelectionJoinTransform, SelectionSequentialTransform\n",
        "\n",
        "PATH = '/content/Poly-Encoder/chatbot_output/poly_16_pytorch_model.bin'\n",
        "\n",
        "bert_name = 'klue/bert-base'\n",
        "bert_config = BertConfig.from_pretrained(bert_name)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "\n",
        "context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=256)\n",
        "response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=128)\n",
        "\n",
        "bert = BertModel.from_pretrained(bert_name, config=bert_config)\n",
        "\n",
        "model = PolyEncoder(bert_config, bert=bert, poly_m=16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rapB1afogMhI"
      },
      "outputs": [],
      "source": [
        "context = ['This framework generates embeddings for each input sentence', \n",
        "           'Sentences are passed as a list of string.', \n",
        "           'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "candidates = ['This framework generates embeddings for each input sentence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FAPphUhb-jp"
      },
      "outputs": [],
      "source": [
        "context_input_ids, context_input_masks = context_transform(context)\n",
        "responses_token_ids_list, responses_input_masks_list = response_transform(candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9CP9rzow1ec"
      },
      "outputs": [],
      "source": [
        "contexts_token_ids_list_batch, contexts_input_masks_list_batch, \\\n",
        "responses_token_ids_list_batch, responses_input_masks_list_batch = [context_input_ids], [context_input_masks], [responses_token_ids_list], [responses_input_masks_list]\n",
        "\n",
        "long_tensors = [contexts_token_ids_list_batch, contexts_input_masks_list_batch,\n",
        "                                            responses_token_ids_list_batch, responses_input_masks_list_batch]\n",
        "\n",
        "contexts_token_ids_list_batch, contexts_input_masks_list_batch, \\\n",
        "responses_token_ids_list_batch, responses_input_masks_list_batch = (torch.tensor(t, dtype=torch.long) for t in long_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward\n",
        "model(contexts_token_ids_list_batch, contexts_input_masks_list_batch, responses_token_ids_list_batch, responses_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "IwPAL7K4ymBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_ELL57RV6vb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3c3f6c93-a4e9-4b8d-b635-bf5440ae8b77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5ef7dc10c6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mctx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts_token_ids_list_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts_input_masks_list_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [bs, length, dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpoly_code_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts_token_ids_list_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "def embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        ctx_out = model.bert(contexts_token_ids_list_batch, contexts_input_masks_list_batch)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(model.poly_m, dtype=torch.long).to(contexts_token_ids_list_batch.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(1, model.poly_m)\n",
        "        poly_codes = model.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = model.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        return embs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embs = embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "IG58NhFdYW8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "                \n",
        "        batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape # res_cnt is 1 during training\n",
        "        responses_token_ids_list_batch = responses_token_ids_list_batch.view(-1, seq_length)\n",
        "        responses_input_masks_list_batch = responses_input_masks_list_batch.view(-1, seq_length)\n",
        "        cand_emb = model.bert(responses_token_ids_list_batch, responses_input_masks_list_batch)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        return cand_emb"
      ],
      "metadata": {
        "id": "vwA8KNRm9_K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cand_emb = cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "TWCWgALsYZN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(embs, cand_emb):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "        dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "        \n",
        "        return dot_product"
      ],
      "metadata": {
        "id": "B47Tw6vTKYr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score(embs, cand_emb)"
      ],
      "metadata": {
        "id": "JnKdzX6JYifc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7AEttyhXCLF",
        "outputId": "74bf8fe1-1205-42de-fe9d-a5b76aba46cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 768])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QogL8-GqSwMs"
      },
      "outputs": [],
      "source": [
        "model.dot_attention()"
      ]
    }
  ]
}